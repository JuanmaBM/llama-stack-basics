{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bda2fed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llama3.2:3b-instruct-fp16\n",
      "Here's one:\n",
      "\n",
      "A man walked into a library and asked the librarian, \"Do you have any books on Pavlov's dogs and SchrÃ¶dinger's cat?\"\n",
      "\n",
      "The librarian replied, \"It rings a bell, but I'm not sure if it's here or not.\"\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(base_url=\"http://localhost:8321\")\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Select the first LLM\n",
    "llm = next(m for m in models if m.model_type == \"llm\")\n",
    "model_id = llm.identifier\n",
    "\n",
    "print(\"Model:\", model_id)\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me something funny\"},\n",
    "    ],\n",
    ")\n",
    "print(response.completion_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7374d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response...\n",
      "\u001b[33minference> \u001b[0m\u001b[33mHere\u001b[0m\u001b[33m's\u001b[0m\u001b[33m a\u001b[0m\u001b[33m brief\u001b[0m\u001b[33m technical\u001b[0m\u001b[33m overview\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Large\u001b[0m\u001b[33m Language\u001b[0m\u001b[33m Models\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mMs\u001b[0m\u001b[33m):\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mWhat\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m Large\u001b[0m\u001b[33m Language\u001b[0m\u001b[33m Model\u001b[0m\u001b[33m?\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mA\u001b[0m\u001b[33m Large\u001b[0m\u001b[33m Language\u001b[0m\u001b[33m Model\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mM\u001b[0m\u001b[33m)\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m type\u001b[0m\u001b[33m of\u001b[0m\u001b[33m artificial\u001b[0m\u001b[33m intelligence\u001b[0m\u001b[33m (\u001b[0m\u001b[33mAI\u001b[0m\u001b[33m)\u001b[0m\u001b[33m model\u001b[0m\u001b[33m that\u001b[0m\u001b[33m uses\u001b[0m\u001b[33m deep\u001b[0m\u001b[33m learning\u001b[0m\u001b[33m techniques\u001b[0m\u001b[33m to\u001b[0m\u001b[33m process\u001b[0m\u001b[33m and\u001b[0m\u001b[33m generate\u001b[0m\u001b[33m human\u001b[0m\u001b[33m-like\u001b[0m\u001b[33m language\u001b[0m\u001b[33m.\u001b[0m\u001b[33m These\u001b[0m\u001b[33m models\u001b[0m\u001b[33m are\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m on\u001b[0m\u001b[33m vast\u001b[0m\u001b[33m amounts\u001b[0m\u001b[33m of\u001b[0m\u001b[33m text\u001b[0m\u001b[33m data\u001b[0m\u001b[33m,\u001b[0m\u001b[33m which\u001b[0m\u001b[33m enables\u001b[0m\u001b[33m them\u001b[0m\u001b[33m to\u001b[0m\u001b[33m learn\u001b[0m\u001b[33m patterns\u001b[0m\u001b[33m,\u001b[0m\u001b[33m relationships\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m structures\u001b[0m\u001b[33m within\u001b[0m\u001b[33m language\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mArchitecture\u001b[0m\u001b[33m:\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m architecture\u001b[0m\u001b[33m of\u001b[0m\u001b[33m an\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m typically\u001b[0m\u001b[33m consists\u001b[0m\u001b[33m of\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mEncoder\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m This\u001b[0m\u001b[33m is\u001b[0m\u001b[33m the\u001b[0m\u001b[33m input\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m that\u001b[0m\u001b[33m processes\u001b[0m\u001b[33m the\u001b[0m\u001b[33m text\u001b[0m\u001b[33m data\u001b[0m\u001b[33m into\u001b[0m\u001b[33m a\u001b[0m\u001b[33m numerical\u001b[0m\u001b[33m representation\u001b[0m\u001b[33m,\u001b[0m\u001b[33m known\u001b[0m\u001b[33m as\u001b[0m\u001b[33m embeddings\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mTransformer\u001b[0m\u001b[33m Encoder\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m This\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-layer\u001b[0m\u001b[33med\u001b[0m\u001b[33m neural\u001b[0m\u001b[33m network\u001b[0m\u001b[33m that\u001b[0m\u001b[33m uses\u001b[0m\u001b[33m self\u001b[0m\u001b[33m-\u001b[0m\u001b[33mattention\u001b[0m\u001b[33m mechanisms\u001b[0m\u001b[33m to\u001b[0m\u001b[33m process\u001b[0m\u001b[33m the\u001b[0m\u001b[33m embedded\u001b[0m\u001b[33m representations\u001b[0m\u001b[33m and\u001b[0m\u001b[33m generate\u001b[0m\u001b[33m contextual\u001b[0m\u001b[33mized\u001b[0m\u001b[33m features\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mDecoder\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m This\u001b[0m\u001b[33m output\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m generates\u001b[0m\u001b[33m the\u001b[0m\u001b[33m final\u001b[0m\u001b[33m response\u001b[0m\u001b[33m based\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m context\u001b[0m\u001b[33m and\u001b[0m\u001b[33m the\u001b[0m\u001b[33m input\u001b[0m\u001b[33m prompt\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mKey\u001b[0m\u001b[33m Components\u001b[0m\u001b[33m:\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mSelf\u001b[0m\u001b[33m-\u001b[0m\u001b[33mAttention\u001b[0m\u001b[33m Mechan\u001b[0m\u001b[33mism\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Allows\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m to\u001b[0m\u001b[33m weigh\u001b[0m\u001b[33m the\u001b[0m\u001b[33m importance\u001b[0m\u001b[33m of\u001b[0m\u001b[33m different\u001b[0m\u001b[33m parts\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m input\u001b[0m\u001b[33m text\u001b[0m\u001b[33m when\u001b[0m\u001b[33m generating\u001b[0m\u001b[33m the\u001b[0m\u001b[33m output\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mMulti\u001b[0m\u001b[33m-\u001b[0m\u001b[33mHead\u001b[0m\u001b[33m Attention\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Enables\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m to\u001b[0m\u001b[33m attend\u001b[0m\u001b[33m to\u001b[0m\u001b[33m multiple\u001b[0m\u001b[33m aspects\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m input\u001b[0m\u001b[33m simultaneously\u001b[0m\u001b[33m,\u001b[0m\u001b[33m improving\u001b[0m\u001b[33m its\u001b[0m\u001b[33m ability\u001b[0m\u001b[33m to\u001b[0m\u001b[33m capture\u001b[0m\u001b[33m complex\u001b[0m\u001b[33m relationships\u001b[0m\u001b[33m between\u001b[0m\u001b[33m words\u001b[0m\u001b[33m and\u001b[0m\u001b[33m phrases\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mPosition\u001b[0m\u001b[33mal\u001b[0m\u001b[33m Encoding\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Adds\u001b[0m\u001b[33m positional\u001b[0m\u001b[33m information\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m input\u001b[0m\u001b[33m embeddings\u001b[0m\u001b[33m to\u001b[0m\u001b[33m preserve\u001b[0m\u001b[33m the\u001b[0m\u001b[33m order\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m words\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m sequence\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mTraining\u001b[0m\u001b[33m:\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mMs\u001b[0m\u001b[33m are\u001b[0m\u001b[33m typically\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m using\u001b[0m\u001b[33m a\u001b[0m\u001b[33m masked\u001b[0m\u001b[33m language\u001b[0m\u001b[33m modeling\u001b[0m\u001b[33m objective\u001b[0m\u001b[33m,\u001b[0m\u001b[33m where\u001b[0m\u001b[33m some\u001b[0m\u001b[33m input\u001b[0m\u001b[33m tokens\u001b[0m\u001b[33m are\u001b[0m\u001b[33m randomly\u001b[0m\u001b[33m replaced\u001b[0m\u001b[33m with\u001b[0m\u001b[33m a\u001b[0m\u001b[33m [\u001b[0m\u001b[33mMASK\u001b[0m\u001b[33m]\u001b[0m\u001b[33m token\u001b[0m\u001b[33m.\u001b[0m\u001b[33m The\u001b[0m\u001b[33m model\u001b[0m\u001b[33m is\u001b[0m\u001b[33m then\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m to\u001b[0m\u001b[33m predict\u001b[0m\u001b[33m the\u001b[0m\u001b[33m original\u001b[0m\u001b[33m token\u001b[0m\u001b[33m that\u001b[0m\u001b[33m was\u001b[0m\u001b[33m replaced\u001b[0m\u001b[33m.\u001b[0m\u001b[33m This\u001b[0m\u001b[33m process\u001b[0m\u001b[33m is\u001b[0m\u001b[33m repeated\u001b[0m\u001b[33m for\u001b[0m\u001b[33m millions\u001b[0m\u001b[33m of\u001b[0m\u001b[33m times\u001b[0m\u001b[33m,\u001b[0m\u001b[33m allowing\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m to\u001b[0m\u001b[33m learn\u001b[0m\u001b[33m patterns\u001b[0m\u001b[33m and\u001b[0m\u001b[33m relationships\u001b[0m\u001b[33m within\u001b[0m\u001b[33m the\u001b[0m\u001b[33m data\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mAdv\u001b[0m\u001b[33mantages\u001b[0m\u001b[33m:\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mImproved\u001b[0m\u001b[33m Language\u001b[0m\u001b[33m Understanding\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m can\u001b[0m\u001b[33m capture\u001b[0m\u001b[33m complex\u001b[0m\u001b[33m nuances\u001b[0m\u001b[33m of\u001b[0m\u001b[33m language\u001b[0m\u001b[33m,\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m idi\u001b[0m\u001b[33moms\u001b[0m\u001b[33m,\u001b[0m\u001b[33m sarc\u001b[0m\u001b[33masm\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m figur\u001b[0m\u001b[33mative\u001b[0m\u001b[33m language\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mContext\u001b[0m\u001b[33mual\u001b[0m\u001b[33mized\u001b[0m\u001b[33m Represent\u001b[0m\u001b[33mations\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m self\u001b[0m\u001b[33m-\u001b[0m\u001b[33mattention\u001b[0m\u001b[33m mechanism\u001b[0m\u001b[33m allows\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m to\u001b[0m\u001b[33m generate\u001b[0m\u001b[33m contextual\u001b[0m\u001b[33mized\u001b[0m\u001b[33m representations\u001b[0m\u001b[33m of\u001b[0m\u001b[33m words\u001b[0m\u001b[33m and\u001b[0m\u001b[33m phrases\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mSc\u001b[0m\u001b[33mal\u001b[0m\u001b[33mability\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m on\u001b[0m\u001b[33m large\u001b[0m\u001b[33m datasets\u001b[0m\u001b[33m,\u001b[0m\u001b[33m making\u001b[0m\u001b[33m them\u001b[0m\u001b[33m suitable\u001b[0m\u001b[33m for\u001b[0m\u001b[33m a\u001b[0m\u001b[33m wide\u001b[0m\u001b[33m range\u001b[0m\u001b[33m of\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mApplications\u001b[0m\u001b[33m:\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mLanguage\u001b[0m\u001b[33m Translation\u001b[0m\u001b[33m**\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mText\u001b[0m\u001b[33m Generation\u001b[0m\u001b[33m**\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mQuestion\u001b[0m\u001b[33m Answer\u001b[0m\u001b[33ming\u001b[0m\u001b[33m**\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mChat\u001b[0m\u001b[33mbots\u001b[0m\u001b[33m and\u001b[0m\u001b[33m Virtual\u001b[0m\u001b[33m Assist\u001b[0m\u001b[33mants\u001b[0m\u001b[33m**\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mSent\u001b[0m\u001b[33miment\u001b[0m\u001b[33m Analysis\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mI\u001b[0m\u001b[33m hope\u001b[0m\u001b[33m this\u001b[0m\u001b[33m gives\u001b[0m\u001b[33m you\u001b[0m\u001b[33m a\u001b[0m\u001b[33m good\u001b[0m\u001b[33m overview\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Large\u001b[0m\u001b[33m Language\u001b[0m\u001b[33m Models\u001b[0m\u001b[33m!\u001b[0m\u001b[33m Do\u001b[0m\u001b[33m you\u001b[0m\u001b[33m have\u001b[0m\u001b[33m any\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m questions\u001b[0m\u001b[33m or\u001b[0m\u001b[33m would\u001b[0m\u001b[33m you\u001b[0m\u001b[33m like\u001b[0m\u001b[33m me\u001b[0m\u001b[33m to\u001b[0m\u001b[33m elaborate\u001b[0m\u001b[33m on\u001b[0m\u001b[33m any\u001b[0m\u001b[33m of\u001b[0m\u001b[33m these\u001b[0m\u001b[33m points\u001b[0m\u001b[33m?\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "import uuid\n",
    "\n",
    "client = LlamaStackClient(base_url=f\"http://localhost:8321\")\n",
    "\n",
    "models = client.models.list()\n",
    "llm = next(m for m in models if m.model_type == \"llm\")\n",
    "model_id = llm.identifier\n",
    "\n",
    "agent = Agent(client, model=model_id, instructions=\"You are a helpful assistant.\")\n",
    "\n",
    "s_id = agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
    "\n",
    "print(\"Streaming response...\")\n",
    "stream = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Give me a short technical overview of LLM\"}], session_id=s_id, stream=True\n",
    ")\n",
    "for event in AgentEventLogger().log(stream):\n",
    "    event.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4609cfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user> what is torchtune\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[\u001b[0m\u001b[33mknowledge\u001b[0m\u001b[33m_search\u001b[0m\u001b[33m(query\u001b[0m\u001b[33m='\u001b[0m\u001b[33mtorch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m')]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'torchtune'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text='Result 1:\\nDocument_id:num-1\\nContent:  conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. For any\\ncustom local dataset we always need to specify ``source``, ``data_files``, and ``split`` for any dataset\\nbuilder in torchtune. For :func:`~torchtune.datasets.chat_dataset`, we additionally need to specify\\n``conversation_column`` and ``conversation_style``. Our data follows the ``\"sharegpt\"`` format, so\\nwe can specify that here. Altogether, our :func:`~torchtune.datasets.chat_dataset` call should\\nlook like so:\\n\\n.. code-block:: python\\n\\n    from torchtune.datasets import chat_dataset\\n    from torchtune.models.llama3 import llama3_tokenizer\\n\\n    tokenizer = llama3_tokenizer(\"/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\")\\n    ds = chat_dataset(\\n        tokenizer=tokenizer,\\n        source=\"json\",\\n        data_files=\"data/my_data.json\",\\n        split=\"train\",\\n        conversation_column=\"dialogue\",\\n        conversation_style=\"sharegpt\",\\n    )\\n\\n.. code-block:: yaml\\n\\n    # In config\\n    tokenizer:\\n      _component_: torchtune.models.llama3.llama3_tokenizer\\n      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\\n\\n    dataset:\\n      _component_: torchtune.datasets.chat_dataset\\n      source: json\\n      data_files: data/my_data.json\\n      split: train\\n      conversation_column: dialogue\\n      conversation_style: sharegpt\\n\\n.. note::\\n    You can pass in any keyword argument for `load_dataset <https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset>`_ into all our\\n    Dataset classes and they will honor them. This is useful for common parameters\\n    such as specifying the data split with :code:`split` or configuration with\\n    :code:`name`\\n\\nIf you needed to add a prompt template, you would simply pass it into the tokenizer.\\nSince we\\'re fine-tuning Llama3, the tokenizer will handle all formatting for\\nus and prompt templates are optional. Other models such as Mistral\\'s :class:`~torchtune.models.mistral._tokenizer.MistralTokenizer`,\\nuse a chat template by default (:class:`~torchtune.models.mistral.MistralChatTemplate`) to format\\nall messages according to their `recommendations <https://\\n', type='text'), TextContentItem(text='Result 2:\\nDocument_id:num-4\\nContent:  LoRA to Llama2 models\\n------------------------------\\n\\nWith torchtune, we can easily apply LoRA to Llama2 with a variety of different configurations.\\nLet\\'s take a look at how to construct Llama2 models in torchtune with and without LoRA.\\n\\n.. code-block:: python\\n\\n  from torchtune.models.llama2 import llama2_7b, lora_llama2_7b\\n\\n  # Build Llama2 without any LoRA layers\\n  base_model = llama2_7b()\\n\\n  # The default settings for lora_llama2_7b will match those for llama2_7b\\n  # We just need to define which layers we want LoRA applied to.\\n  # Within each self-attention, we can choose from [\"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\"].\\n  # We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\\n  # layers outside of the self-attention.\\n  lora_model = lora_llama2_7b(lora_attn_modules=[\"q_proj\", \"v_proj\"])\\n\\n.. note::\\n\\n    Calling :func:`lora_llama_2_7b <torchtune.models.llama2.lora_llama2_7b>` alone will not handle the definition of which parameters are trainable.\\n    See :ref:`below<setting_trainable_params>` for how to do this.\\n\\nLet\\'s inspect each of these models a bit more closely.\\n\\n.. code-block:: bash\\n\\n  # Print the first layer\\'s self-attention in the usual Llama2 model\\n  >>> print(base_model.layers[0].attn)\\n  MultiHeadAttention(\\n    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (pos_embeddings): RotaryPositionalEmbeddings()\\n  )\\n\\n  # Print the same for Llama2 with LoRA weights\\n  >>> print(lora_model.layers[0].attn)\\n  MultiHeadAttention(\\n    (q_proj): LoRALinear(\\n      (dropout): Dropout(p=0.0, inplace=False)\\n     \\n', type='text'), TextContentItem(text=\"Result 3:\\nDocument_id:num-4\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\", type='text'), TextContentItem(text='Result 4:\\nDocument_id:num-0\\nContent: precision\", uses 4 bytes per model and optimizer parameter.\\n* ``bfloat16``, referred to as \"half-precision\", uses 2 bytes per model and optimizer parameter - effectively half\\n  the memory of ``fp32``, and also improves training speed. Generally, if your hardware supports training with ``bfloat16``,\\n  we recommend using it - this is the default setting for our recipes.\\n\\n.. note::\\n\\n  Another common paradigm is \"mixed-precision\" training: where model weights are in ``bfloat16`` (or ``fp16``), and optimizer\\n  states are in ``fp32``. Currently, we don\\'t support mixed-precision training in torchtune.\\n\\n*Sounds great! How do I use it?*\\n\\nSimply use the ``dtype`` flag or config entry in all our recipes! For example, to use half-precision training in ``bf16``,\\nset ``dtype=bf16``.\\n\\n.. _glossary_act_ckpt:\\n\\nActivation Checkpointing\\n------------------------\\n\\n*What\\'s going on here?*\\n\\nThe relevant section in the `PyTorch documentation <https://pytorch.org/docs/stable/checkpoint.html>`_ explains this concept well.\\nTo quote:\\n\\n  Activation checkpointing is a technique that trades compute for memory.\\n  Instead of keeping tensors needed for backward alive until they are used in\\n  gradient computation during backward, forward computation in checkpointed\\n  regions omits saving tensors for backward and recomputes them during the backward pass.\\n\\nThis setting is helpful for when you\\'re memory-constrained, especially due to larger batch sizes or longer context lengths.\\nHowever, these savings in memory come at the cost of training speed (i.e. tokens-per-second),\\nand in most cases training can slow down quite a bit as a result of this activation recomputation.\\n\\n*Sounds great! How do I use it?*\\n\\nTo enable activation checkpointing, use ``enable_activation_checkpointing=True``.\\n\\n.. _glossary_act_off:\\n\\nActivation Offloading\\n---------------------\\n\\n*What\\'s going on here?*\\n\\nYou may have just read about activation checkpointing! Similar to checkpointing, offloading is a memory\\nefficiency technique that allows saving GPU VRAM by temporarily moving activations to CPU and bringing\\nthem back when needed in the backward pass.\\n\\nSee `PyTorch autograd hook tutorial <https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html#saving-tensors-to-cpu>`_\\nfor more details about how this is implemented through :\\n', type='text'), TextContentItem(text='Result 5:\\nDocument_id:num-4\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n', type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"torchtune\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mThis\u001b[0m\u001b[33m text\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m for\u001b[0m\u001b[33m torch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m,\u001b[0m\u001b[33m a\u001b[0m\u001b[33m library\u001b[0m\u001b[33m used\u001b[0m\u001b[33m for\u001b[0m\u001b[33m hyper\u001b[0m\u001b[33mparameter\u001b[0m\u001b[33m tuning\u001b[0m\u001b[33m and\u001b[0m\u001b[33m model\u001b[0m\u001b[33m selection\u001b[0m\u001b[33m.\u001b[0m\u001b[33m It\u001b[0m\u001b[33m provides\u001b[0m\u001b[33m various\u001b[0m\u001b[33m techniques\u001b[0m\u001b[33m for\u001b[0m\u001b[33m optimizing\u001b[0m\u001b[33m models\u001b[0m\u001b[33m,\u001b[0m\u001b[33m including\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLow\u001b[0m\u001b[33m-R\u001b[0m\u001b[33mank\u001b[0m\u001b[33m Adapt\u001b[0m\u001b[33mation\u001b[0m\u001b[33m)\u001b[0m\u001b[33m fin\u001b[0m\u001b[33met\u001b[0m\u001b[33muning\u001b[0m\u001b[33m,\u001b[0m\u001b[33m activation\u001b[0m\u001b[33m checkpoint\u001b[0m\u001b[33ming\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m activation\u001b[0m\u001b[33m off\u001b[0m\u001b[33mloading\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m main\u001b[0m\u001b[33m topics\u001b[0m\u001b[33m covered\u001b[0m\u001b[33m in\u001b[0m\u001b[33m this\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m include\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mLo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m Fin\u001b[0m\u001b[33met\u001b[0m\u001b[33muning\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m A\u001b[0m\u001b[33m technique\u001b[0m\u001b[33m that\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33munes\u001b[0m\u001b[33m a\u001b[0m\u001b[33m pre\u001b[0m\u001b[33m-trained\u001b[0m\u001b[33m model\u001b[0m\u001b[33m by\u001b[0m\u001b[33m adapting\u001b[0m\u001b[33m the\u001b[0m\u001b[33m low\u001b[0m\u001b[33m-r\u001b[0m\u001b[33mank\u001b[0m\u001b[33m adaptation\u001b[0m\u001b[33m matrix\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mActivation\u001b[0m\u001b[33m Check\u001b[0m\u001b[33mpoint\u001b[0m\u001b[33ming\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m A\u001b[0m\u001b[33m technique\u001b[0m\u001b[33m that\u001b[0m\u001b[33m trades\u001b[0m\u001b[33m compute\u001b[0m\u001b[33m for\u001b[0m\u001b[33m memory\u001b[0m\u001b[33m by\u001b[0m\u001b[33m recom\u001b[0m\u001b[33mput\u001b[0m\u001b[33ming\u001b[0m\u001b[33m tensors\u001b[0m\u001b[33m during\u001b[0m\u001b[33m backward\u001b[0m\u001b[33m pass\u001b[0m\u001b[33m instead\u001b[0m\u001b[33m of\u001b[0m\u001b[33m saving\u001b[0m\u001b[33m them\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mActivation\u001b[0m\u001b[33m Off\u001b[0m\u001b[33mloading\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Similar\u001b[0m\u001b[33m to\u001b[0m\u001b[33m checkpoint\u001b[0m\u001b[33ming\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m moves\u001b[0m\u001b[33m activations\u001b[0m\u001b[33m to\u001b[0m\u001b[33m CPU\u001b[0m\u001b[33m and\u001b[0m\u001b[33m brings\u001b[0m\u001b[33m them\u001b[0m\u001b[33m back\u001b[0m\u001b[33m when\u001b[0m\u001b[33m needed\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m backward\u001b[0m\u001b[33m pass\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m also\u001b[0m\u001b[33m provides\u001b[0m\u001b[33m a\u001b[0m\u001b[33m recipe\u001b[0m\u001b[33m for\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m fin\u001b[0m\u001b[33met\u001b[0m\u001b[33muning\u001b[0m\u001b[33m using\u001b[0m\u001b[33m torch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m's\u001b[0m\u001b[33m `\u001b[0m\u001b[33mLo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m recipe\u001b[0m\u001b[33m <https\u001b[0m\u001b[33m://\u001b[0m\u001b[33mgithub\u001b[0m\u001b[33m.com\u001b[0m\u001b[33m/py\u001b[0m\u001b[33mtorch\u001b[0m\u001b[33m/t\u001b[0m\u001b[33mor\u001b[0m\u001b[33mcht\u001b[0m\u001b[33mune\u001b[0m\u001b[33m/blob\u001b[0m\u001b[33m/\u001b[0m\u001b[33m486\u001b[0m\u001b[33m26\u001b[0m\u001b[33md\u001b[0m\u001b[33m19\u001b[0m\u001b[33md\u001b[0m\u001b[33m210\u001b[0m\u001b[33m8\u001b[0m\u001b[33mf\u001b[0m\u001b[33m92\u001b[0m\u001b[33mc\u001b[0m\u001b[33m749\u001b[0m\u001b[33m411\u001b[0m\u001b[33mf\u001b[0m\u001b[33mbd\u001b[0m\u001b[33m5\u001b[0m\u001b[33mf\u001b[0m\u001b[33m0\u001b[0m\u001b[33mff\u001b[0m\u001b[33m140\u001b[0m\u001b[33m023\u001b[0m\u001b[33ma\u001b[0m\u001b[33m25\u001b[0m\u001b[33m/\u001b[0m\u001b[33mrecipes\u001b[0m\u001b[33m/l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_f\u001b[0m\u001b[33minet\u001b[0m\u001b[33mune\u001b[0m\u001b[33m.py\u001b[0m\u001b[33m>`\u001b[0m\u001b[33m_.\u001b[0m\u001b[33m This\u001b[0m\u001b[33m recipe\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m used\u001b[0m\u001b[33m to\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33mune\u001b[0m\u001b[33m a\u001b[0m\u001b[33m model\u001b[0m\u001b[33m using\u001b[0m\u001b[33m the\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m technique\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m also\u001b[0m\u001b[33m mentions\u001b[0m\u001b[33m various\u001b[0m\u001b[33m configuration\u001b[0m\u001b[33m options\u001b[0m\u001b[33m,\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_at\u001b[0m\u001b[33mtn\u001b[0m\u001b[33m_modules\u001b[0m\u001b[33m`:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m modules\u001b[0m\u001b[33m to\u001b[0m\u001b[33m adapt\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_rank\u001b[0m\u001b[33m`:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m rank\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m adaptation\u001b[0m\u001b[33m matrix\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_alpha\u001b[0m\u001b[33m`:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m alpha\u001b[0m\u001b[33m value\u001b[0m\u001b[33m for\u001b[0m\u001b[33m the\u001b[0m\u001b[33m adaptation\u001b[0m\u001b[33m matrix\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33mdtype\u001b[0m\u001b[33m`:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m data\u001b[0m\u001b[33m type\u001b[0m\u001b[33m to\u001b[0m\u001b[33m use\u001b[0m\u001b[33m (\u001b[0m\u001b[33me\u001b[0m\u001b[33m.g\u001b[0m\u001b[33m.,\u001b[0m\u001b[33m `\u001b[0m\u001b[33mbf\u001b[0m\u001b[33m16\u001b[0m\u001b[33m`\u001b[0m\u001b[33m for\u001b[0m\u001b[33m half\u001b[0m\u001b[33m-\u001b[0m\u001b[33mprecision\u001b[0m\u001b[33m training\u001b[0m\u001b[33m).\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33menable\u001b[0m\u001b[33m_activation\u001b[0m\u001b[33m_checkpoint\u001b[0m\u001b[33ming\u001b[0m\u001b[33m`:\u001b[0m\u001b[33m A\u001b[0m\u001b[33m flag\u001b[0m\u001b[33m to\u001b[0m\u001b[33m enable\u001b[0m\u001b[33m activation\u001b[0m\u001b[33m checkpoint\u001b[0m\u001b[33ming\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mOverall\u001b[0m\u001b[33m,\u001b[0m\u001b[33m this\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m provides\u001b[0m\u001b[33m a\u001b[0m\u001b[33m comprehensive\u001b[0m\u001b[33m guide\u001b[0m\u001b[33m to\u001b[0m\u001b[33m using\u001b[0m\u001b[33m torch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m for\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m fin\u001b[0m\u001b[33met\u001b[0m\u001b[33muning\u001b[0m\u001b[33m and\u001b[0m\u001b[33m other\u001b[0m\u001b[33m optimization\u001b[0m\u001b[33m techniques\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0muser> tell me about dora\n",
      "\u001b[33minference> \u001b[0m\u001b[33mD\u001b[0m\u001b[33mORA\u001b[0m\u001b[33m is\u001b[0m\u001b[33m an\u001b[0m\u001b[33m acronym\u001b[0m\u001b[33m that\u001b[0m\u001b[33m can\u001b[0m\u001b[33m stand\u001b[0m\u001b[33m for\u001b[0m\u001b[33m several\u001b[0m\u001b[33m things\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m I\u001b[0m\u001b[33m'll\u001b[0m\u001b[33m cover\u001b[0m\u001b[33m the\u001b[0m\u001b[33m most\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m ones\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mD\u001b[0m\u001b[33mORA\u001b[0m\u001b[33m (\u001b[0m\u001b[33mDeep\u001b[0m\u001b[33m Rein\u001b[0m\u001b[33mforcement\u001b[0m\u001b[33m Learning\u001b[0m\u001b[33m from\u001b[0m\u001b[33m Demonstr\u001b[0m\u001b[33mations\u001b[0m\u001b[33m)**\u001b[0m\u001b[33m:\u001b[0m\u001b[33m D\u001b[0m\u001b[33mORA\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m reinforcement\u001b[0m\u001b[33m learning\u001b[0m\u001b[33m algorithm\u001b[0m\u001b[33m developed\u001b[0m\u001b[33m by\u001b[0m\u001b[33m Google\u001b[0m\u001b[33m.\u001b[0m\u001b[33m It\u001b[0m\u001b[33m's\u001b[0m\u001b[33m designed\u001b[0m\u001b[33m to\u001b[0m\u001b[33m learn\u001b[0m\u001b[33m complex\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m by\u001b[0m\u001b[33m observing\u001b[0m\u001b[33m demonstrations\u001b[0m\u001b[33m of\u001b[0m\u001b[33m expert\u001b[0m\u001b[33m behavior\u001b[0m\u001b[33m and\u001b[0m\u001b[33m then\u001b[0m\u001b[33m general\u001b[0m\u001b[33mizing\u001b[0m\u001b[33m to\u001b[0m\u001b[33m new\u001b[0m\u001b[33m situations\u001b[0m\u001b[33m.\u001b[0m\u001b[33m D\u001b[0m\u001b[33mORA\u001b[0m\u001b[33m uses\u001b[0m\u001b[33m a\u001b[0m\u001b[33m combination\u001b[0m\u001b[33m of\u001b[0m\u001b[33m imitation\u001b[0m\u001b[33m learning\u001b[0m\u001b[33m and\u001b[0m\u001b[33m reinforcement\u001b[0m\u001b[33m learning\u001b[0m\u001b[33m techniques\u001b[0m\u001b[33m to\u001b[0m\u001b[33m learn\u001b[0m\u001b[33m policies\u001b[0m\u001b[33m that\u001b[0m\u001b[33m can\u001b[0m\u001b[33m perform\u001b[0m\u001b[33m well\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m environments\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mD\u001b[0m\u001b[33mORA\u001b[0m\u001b[33m (\u001b[0m\u001b[33mData\u001b[0m\u001b[33m-\u001b[0m\u001b[33mDriven\u001b[0m\u001b[33m Optimization\u001b[0m\u001b[33m for\u001b[0m\u001b[33m Recommendation\u001b[0m\u001b[33m Algorithms\u001b[0m\u001b[33m)**\u001b[0m\u001b[33m:\u001b[0m\u001b[33m D\u001b[0m\u001b[33mORA\u001b[0m\u001b[33m is\u001b[0m\u001b[33m also\u001b[0m\u001b[33m an\u001b[0m\u001b[33m optimization\u001b[0m\u001b[33m algorithm\u001b[0m\u001b[33m used\u001b[0m\u001b[33m in\u001b[0m\u001b[33m recommendation\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m,\u001b[0m\u001b[33m particularly\u001b[0m\u001b[33m in\u001b[0m\u001b[33m natural\u001b[0m\u001b[33m language\u001b[0m\u001b[33m processing\u001b[0m\u001b[33m and\u001b[0m\u001b[33m information\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m.\u001b[0m\u001b[33m It\u001b[0m\u001b[33m's\u001b[0m\u001b[33m designed\u001b[0m\u001b[33m to\u001b[0m\u001b[33m optimize\u001b[0m\u001b[33m the\u001b[0m\u001b[33m performance\u001b[0m\u001b[33m of\u001b[0m\u001b[33m recommendation\u001b[0m\u001b[33m algorithms\u001b[0m\u001b[33m by\u001b[0m\u001b[33m iter\u001b[0m\u001b[33matively\u001b[0m\u001b[33m updating\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m based\u001b[0m\u001b[33m on\u001b[0m\u001b[33m user\u001b[0m\u001b[33m feedback\u001b[0m\u001b[33m and\u001b[0m\u001b[33m item\u001b[0m\u001b[33m ratings\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mD\u001b[0m\u001b[33mORA\u001b[0m\u001b[33m (\u001b[0m\u001b[33mDynamic\u001b[0m\u001b[33m Object\u001b[0m\u001b[33m Recognition\u001b[0m\u001b[33m Algorithm\u001b[0m\u001b[33m)**\u001b[0m\u001b[33m:\u001b[0m\u001b[33m In\u001b[0m\u001b[33m computer\u001b[0m\u001b[33m vision\u001b[0m\u001b[33m,\u001b[0m\u001b[33m D\u001b[0m\u001b[33mORA\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m real\u001b[0m\u001b[33m-time\u001b[0m\u001b[33m object\u001b[0m\u001b[33m recognition\u001b[0m\u001b[33m algorithm\u001b[0m\u001b[33m that\u001b[0m\u001b[33m can\u001b[0m\u001b[33m detect\u001b[0m\u001b[33m objects\u001b[0m\u001b[33m in\u001b[0m\u001b[33m images\u001b[0m\u001b[33m or\u001b[0m\u001b[33m videos\u001b[0m\u001b[33m.\u001b[0m\u001b[33m It\u001b[0m\u001b[33m uses\u001b[0m\u001b[33m deep\u001b[0m\u001b[33m learning\u001b[0m\u001b[33m techniques\u001b[0m\u001b[33m,\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m convolution\u001b[0m\u001b[33mal\u001b[0m\u001b[33m neural\u001b[0m\u001b[33m networks\u001b[0m\u001b[33m (\u001b[0m\u001b[33mCNN\u001b[0m\u001b[33ms\u001b[0m\u001b[33m),\u001b[0m\u001b[33m to\u001b[0m\u001b[33m recognize\u001b[0m\u001b[33m objects\u001b[0m\u001b[33m and\u001b[0m\u001b[33m track\u001b[0m\u001b[33m their\u001b[0m\u001b[33m movement\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mIn\u001b[0m\u001b[33m general\u001b[0m\u001b[33m,\u001b[0m\u001b[33m D\u001b[0m\u001b[33mORA\u001b[0m\u001b[33m refers\u001b[0m\u001b[33m to\u001b[0m\u001b[33m any\u001b[0m\u001b[33m system\u001b[0m\u001b[33m or\u001b[0m\u001b[33m algorithm\u001b[0m\u001b[33m that\u001b[0m\u001b[33m aims\u001b[0m\u001b[33m to\u001b[0m\u001b[33m optimize\u001b[0m\u001b[33m performance\u001b[0m\u001b[33m,\u001b[0m\u001b[33m learn\u001b[0m\u001b[33m from\u001b[0m\u001b[33m demonstrations\u001b[0m\u001b[33m,\u001b[0m\u001b[33m or\u001b[0m\u001b[33m recognize\u001b[0m\u001b[33m objects\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m domains\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mIf\u001b[0m\u001b[33m you\u001b[0m\u001b[33m're\u001b[0m\u001b[33m interested\u001b[0m\u001b[33m in\u001b[0m\u001b[33m learning\u001b[0m\u001b[33m more\u001b[0m\u001b[33m about\u001b[0m\u001b[33m a\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m D\u001b[0m\u001b[33mORA\u001b[0m\u001b[33m-related\u001b[0m\u001b[33m topic\u001b[0m\u001b[33m,\u001b[0m\u001b[33m please\u001b[0m\u001b[33m let\u001b[0m\u001b[33m me\u001b[0m\u001b[33m know\u001b[0m\u001b[33m!\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "from llama_stack_client.types import Document\n",
    "import uuid\n",
    "\n",
    "client = LlamaStackClient(base_url=\"http://localhost:8321\")\n",
    "\n",
    "# Create a vector database instance\n",
    "embed_lm = next(m for m in client.models.list() if m.model_type == \"embedding\")\n",
    "embedding_model = embed_lm.identifier\n",
    "vector_db_id = f\"v{uuid.uuid4().hex}\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=embedding_model,\n",
    ")\n",
    "\n",
    "# Create Documents\n",
    "urls = [\n",
    "    \"memory_optimizations.rst\",\n",
    "    \"chat.rst\",\n",
    "    \"llama3.rst\",\n",
    "    \"qat_finetune.rst\",\n",
    "    \"lora_finetune.rst\",\n",
    "]\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "\n",
    "# Insert documents\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512,\n",
    ")\n",
    "\n",
    "# Get the model being served\n",
    "llm = next(m for m in client.models.list() if m.model_type == \"llm\")\n",
    "model = llm.identifier\n",
    "\n",
    "# Create the RAG agent\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=model,\n",
    "    instructions=\"You are a helpful assistant. Use the RAG tool to answer questions as needed.\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\"vector_db_ids\": [vector_db_id]},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "session_id = rag_agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
    "\n",
    "turns = [\"what is torchtune\", \"tell me about dora\"]\n",
    "\n",
    "for t in turns:\n",
    "    print(\"user>\", t)\n",
    "    stream = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": t}], session_id=session_id, stream=True\n",
    "    )\n",
    "    for event in AgentEventLogger().log(stream):\n",
    "        event.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-stack-vjisKARn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
